{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_SINGLE_ANOMALY_FEATURIZED_PATH = \"./Datasets/Featurized/BankChurners/DirtySingleAnomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(f\"{DIRTY_SINGLE_ANOMALY_FEATURIZED_PATH}/DirtySingleAnomaly.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# META MODEL INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.base import clone as clone_estimator\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaNoveltyModel:\n",
    "    def __init__(self, base_clr, base_scaler):\n",
    "        self.base_clr = base_clr\n",
    "        self.base_scaler = base_scaler\n",
    "        self.estimators = dict()\n",
    "        self.scalers = dict()\n",
    "    \n",
    "    def fit(self, X, z):\n",
    "        sorted_by_column = sorted(zip(X, z), key=lambda x: x[-1])\n",
    "        \n",
    "        for (z_val, X_z) in groupby(sorted_by_column, key=lambda x: x[-1]):\n",
    "            X_, _ = zip(*X_z)\n",
    "            estimator = clone_estimator(self.base_clr, safe=True)\n",
    "            scaler = clone_estimator(self.base_scaler, safe=True)\n",
    "            x_ = scaler.fit_transform(X_)\n",
    "            estimator.fit(x_)\n",
    "            self.estimators[z_val] = estimator\n",
    "            self.scalers[z_val] = scaler\n",
    "    \n",
    "    def __predict(self, column, x):\n",
    "        check_is_fitted(self.scalers[column])\n",
    "        check_is_fitted(self.estimators[column])\n",
    "        \n",
    "        return self.estimators[column].predict(\n",
    "            self.scalers[column].transform(\n",
    "                np.atleast_2d(x)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def __decision_function(self, column, x):\n",
    "        check_is_fitted(self.scalers[column])\n",
    "        check_is_fitted(self.estimators[column])\n",
    "        \n",
    "        if(hasattr(self.estimators[column], 'decision_function')):\n",
    "            return self.estimators[column].decision_function(\n",
    "                self.scalers[column].transform(\n",
    "                    np.atleast_2d(x)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(f\"estimator for {column} has no decision_function\")\n",
    "            \n",
    "    def __apply_function(self, X, z, func):\n",
    "        i = list(range(len(z)))\n",
    "        sorted_by_column = sorted(zip(X, i, z), key=lambda x: x[-1])\n",
    "        \n",
    "        pred = np.zeros((len(z)))\n",
    "        \n",
    "        for (z_val, X_z) in groupby(sorted_by_column, key=lambda x: x[-1]):\n",
    "            X_, i_, _ = zip(*X_z)\n",
    "            pred[list(i_)] = func(z_val, X_)\n",
    "            \n",
    "        return np.array(pred)\n",
    "    \n",
    "    def predict(self, X, z):\n",
    "        return self.__apply_function(X, z, self.__predict)\n",
    "    \n",
    "    def decision_function(self, X, z):\n",
    "        return self.__apply_function(X, z, self.__decision_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaOutlierModel:\n",
    "    def __init__(self, base_clr, base_scaler):\n",
    "        self.base_clr = base_clr\n",
    "        self.base_scaler = base_scaler\n",
    "        self.estimators = dict()\n",
    "        self.scalers = dict()\n",
    "    \n",
    "    def __fit_predict(self, column, x):\n",
    "        return self.estimators[column].fit_predict(\n",
    "            self.scalers[column].fit_transform(\n",
    "                np.atleast_2d(x)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def __fit_decision_function(self, column, x):\n",
    "        x_ = self.scalers[column].fit_transform(np.atleast_2d(x))\n",
    "        if(hasattr(self.estimators[column], 'decision_function')):\n",
    "            self.estimators[column].fit(x_)\n",
    "            return self.estimators[column].decision_function(x_)\n",
    "        elif isinstance(self.estimators[column], LocalOutlierFactor):\n",
    "            self.estimators[column].fit_predict(x_)\n",
    "            return self.estimators[column].negative_outlier_factor_\n",
    "        else:    \n",
    "            raise Exception(f\"estimator for {column} has no decision_function\")\n",
    "            \n",
    "    def __fit_apply(self, X, z, func):\n",
    "        i = list(range(len(z)))\n",
    "        sorted_by_column = sorted(zip(X, i, z), key=lambda x: x[-1])\n",
    "        \n",
    "        pred = np.zeros((len(z)))\n",
    "        \n",
    "        for (z_val, X_z) in groupby(sorted_by_column, key=lambda x: x[-1]):\n",
    "            self.estimators[z_val] = clone_estimator(self.base_clr, safe=True)\n",
    "            self.scalers[z_val] = clone_estimator(self.base_scaler, safe=True)\n",
    "            X_, i_, _ = zip(*X_z)\n",
    "            pred[list(i_)] = func(z_val, X_)\n",
    "            \n",
    "        return np.array(pred)\n",
    "    \n",
    "    def fit_predict(self, X, z):\n",
    "        return self.__fit_apply(X, z, self.__fit_predict)\n",
    "    \n",
    "    def fit_decision_function(self, X, z):\n",
    "        return self.__fit_apply(X, z, self.__fit_decision_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_control_novelty = dict()\n",
    "\n",
    "for anomaly, ds_anomaly in dataset['anomaly'].items():\n",
    "    dataset_anomaly = dict()\n",
    "    \n",
    "    dataset_anomaly['X'] = list()\n",
    "    dataset_anomaly['y'] = list()\n",
    "    dataset_anomaly['z'] = list()\n",
    "    \n",
    "    for column, features in ds_anomaly.items():\n",
    "        dataset_anomaly['X'] += features\n",
    "        dataset_anomaly['y'] += [True] * len(features)\n",
    "        dataset_anomaly['z'] += [column] * len(features)\n",
    "    \n",
    "    datasets_control_novelty[anomaly] = dataset_anomaly\n",
    "\n",
    "for anomaly in dataset['anomaly'].keys():\n",
    "    for column, features in dataset['control'].items():\n",
    "        if column in datasets_control_novelty[anomaly]['z']:\n",
    "            datasets_control_novelty[anomaly]['X'] += features\n",
    "            datasets_control_novelty[anomaly]['y'] += [False] * len(features)\n",
    "            datasets_control_novelty[anomaly]['z'] += [column] * len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = {\n",
    "    'X': list(),\n",
    "    'z': list()\n",
    "}\n",
    "\n",
    "for column, features in dataset['train'].items():\n",
    "    dataset_train['X'] += features\n",
    "    dataset_train['z'] += [column] * len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_control_outlier = dict()\n",
    "\n",
    "for anomaly, ds_anomaly in dataset['anomaly'].items():\n",
    "    dataset_anomaly = dict()\n",
    "    \n",
    "    dataset_anomaly['X'] = list()\n",
    "    dataset_anomaly['y'] = list()\n",
    "    dataset_anomaly['z'] = list()\n",
    "    \n",
    "    for column, features in ds_anomaly.items():\n",
    "        dataset_anomaly['X'] += features\n",
    "        dataset_anomaly['y'] += [True] * len(features)\n",
    "        dataset_anomaly['z'] += [column] * len(features)\n",
    "    \n",
    "    datasets_control_outlier[anomaly] = dataset_anomaly\n",
    "\n",
    "for anomaly in dataset['anomaly'].keys():\n",
    "    for column, features in dataset['control'].items():\n",
    "        if column in datasets_control_outlier[anomaly]['z']:\n",
    "            datasets_control_outlier[anomaly]['X'] += features\n",
    "            datasets_control_outlier[anomaly]['y'] += [False] * len(features)\n",
    "            datasets_control_outlier[anomaly]['z'] += [column] * len(features)\n",
    "            \n",
    "            datasets_control_outlier[anomaly]['X'] += dataset['train'][column]\n",
    "            datasets_control_outlier[anomaly]['y'] += [False] * len(dataset['train'][column])\n",
    "            datasets_control_outlier[anomaly]['z'] += [column] * len(dataset['train'][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN / EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as ss\n",
    "\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOMALY DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_novelty = {\n",
    "    'LocalOutlierFactorNovelty': LocalOutlierFactor(novelty=True),\n",
    "    'OneClassSVM': OneClassSVM(),\n",
    "}\n",
    "\n",
    "algorithms_outlier = {\n",
    "    'LocalOutlierFactor': LocalOutlierFactor(novelty=False),\n",
    "    'IsolationForest': IsolationForest()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_dataset(dataset, rng, share = 0.1):\n",
    "    _, counts = np.unique(dataset['y'], return_counts=True)\n",
    "            \n",
    "    positive_loc = np.argwhere(np.array(dataset['y']) == True).squeeze()\n",
    "    negative_loc = np.argwhere(np.array(dataset['y']) == False).squeeze()\n",
    "\n",
    "    num_negatives = counts[0]\n",
    "    num_positives = int(num_negatives / (1 - share) - num_negatives)\n",
    "    new_positives = rng.choice(positive_loc, num_positives)\n",
    "\n",
    "    X_control_negatives = np.array(dataset['X'], dtype='object')[negative_loc].tolist()\n",
    "    Y_control_negatives = np.array(dataset['y'], dtype='object')[negative_loc].tolist()\n",
    "    Z_control_negatives = np.array(dataset['z'], dtype='object')[negative_loc].tolist()\n",
    "\n",
    "    X_control_positives = np.array(dataset['X'], dtype='object')[new_positives].tolist()\n",
    "    Y_control_positives = np.array(dataset['y'], dtype='object')[new_positives].tolist()\n",
    "    Z_control_positives = np.array(dataset['z'], dtype='object')[new_positives].tolist()\n",
    "\n",
    "    X_control_new = X_control_negatives + X_control_positives\n",
    "    Y_control_new = np.r_[Y_control_negatives, Y_control_positives]\n",
    "    Z_control_new = Z_control_negatives + Z_control_positives\n",
    "    \n",
    "    return {\n",
    "        'X': X_control_new,\n",
    "        'y': Y_control_new,\n",
    "        'z': Z_control_new\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "for algorithm_name, algorithm in algorithms_novelty.items():\n",
    "    clr = MetaNoveltyModel(algorithm, StandardScaler())\n",
    "    clr.fit(dataset_train['X'], dataset_train['z'])\n",
    "    \n",
    "    for anomaly, dataset in datasets_control_novelty.items():\n",
    "        rng = np.random.default_rng(seed=42)\n",
    "        for i in range(NUM_ITERATIONS):\n",
    "            new_dataset = sample_from_dataset(dataset, rng, 0.1)\n",
    "            \n",
    "            preds = clr.predict(new_dataset['X'], new_dataset['z'])\n",
    "            score = clr.decision_function(new_dataset['X'], new_dataset['z'])\n",
    "\n",
    "            results.append({\n",
    "                'algorithm': algorithm_name,\n",
    "                'anomaly': anomaly,\n",
    "                'iteration': i,\n",
    "                'score': score,\n",
    "                'pred': (preds == -1).astype(int),\n",
    "                'true': new_dataset['y']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm_name, algorithm in algorithms_outlier.items(): \n",
    "    for anomaly, dataset in datasets_control_outlier.items():\n",
    "        rng = np.random.default_rng(seed=42)\n",
    "        for i in range(NUM_ITERATIONS):\n",
    "            new_dataset = sample_from_dataset(dataset, rng, 0.1)\n",
    "            \n",
    "            clr = MetaOutlierModel(algorithm, StandardScaler())\n",
    "\n",
    "            preds = clr.fit_predict(new_dataset['X'], new_dataset['z'])\n",
    "            score = clr.fit_decision_function(new_dataset['X'], new_dataset['z'])\n",
    "\n",
    "            results.append({\n",
    "                'algorithm': algorithm_name,\n",
    "                'anomaly': anomaly,\n",
    "                'iteration': i,\n",
    "                'score': score,\n",
    "                'pred': (preds == -1).astype(int),\n",
    "                'true': new_dataset['y']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = sorted(results, key=lambda x: f\"{x['algorithm']}|{x['anomaly']}\")\n",
    "\n",
    "results_new = list()\n",
    "\n",
    "for gr, grp in groupby(results_sorted, lambda x: f\"{x['algorithm']}|{x['anomaly']}\"):\n",
    "    grp = list(grp)\n",
    "    results_new.append({\n",
    "        'algorithm': gr.split('|')[0],\n",
    "        'anomaly': gr.split('|')[1],\n",
    "        'precision': np.mean([precision_score(result['true'], result['pred']) for result in grp]),\n",
    "        'recall':np.mean([recall_score(result['true'], result['pred']) for result in grp]),\n",
    "        'auc': np.mean([roc_auc_score(result['true'], result['pred']) for result in grp])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>categorical_category_miss</td>\n",
       "      <td>0.494899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>categorical_category_miss</td>\n",
       "      <td>0.673954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>categorical_category_miss</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>categorical_category_miss</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>categorical_distribution_changed</td>\n",
       "      <td>0.498601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>categorical_distribution_changed</td>\n",
       "      <td>0.661667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>categorical_distribution_changed</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>categorical_distribution_changed</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>categorical_new_category_influx</td>\n",
       "      <td>0.434449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>categorical_new_category_influx</td>\n",
       "      <td>0.682456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>categorical_new_category_influx</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>categorical_new_category_influx</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>duplicates_influx</td>\n",
       "      <td>0.620376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>duplicates_influx</td>\n",
       "      <td>0.667995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>duplicates_influx</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>duplicates_influx</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>nan_influx</td>\n",
       "      <td>0.397200</td>\n",
       "      <td>0.929545</td>\n",
       "      <td>0.887148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>nan_influx</td>\n",
       "      <td>0.728485</td>\n",
       "      <td>0.751136</td>\n",
       "      <td>0.860318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>nan_influx</td>\n",
       "      <td>0.847417</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.874318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>nan_influx</td>\n",
       "      <td>0.155324</td>\n",
       "      <td>0.986364</td>\n",
       "      <td>0.698182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>numeric_mean_change</td>\n",
       "      <td>0.481365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>numeric_mean_change</td>\n",
       "      <td>0.862390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>numeric_mean_change</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>numeric_mean_change</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>numeric_variance_change</td>\n",
       "      <td>0.506076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>numeric_variance_change</td>\n",
       "      <td>0.829049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>numeric_variance_change</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>numeric_variance_change</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IsolationForest</td>\n",
       "      <td>random_influx</td>\n",
       "      <td>0.439949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LocalOutlierFactor</td>\n",
       "      <td>random_influx</td>\n",
       "      <td>0.740977</td>\n",
       "      <td>0.901136</td>\n",
       "      <td>0.933318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LocalOutlierFactorNovelty</td>\n",
       "      <td>random_influx</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>random_influx</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    algorithm                           anomaly  precision  \\\n",
       "0             IsolationForest         categorical_category_miss   0.494899   \n",
       "16         LocalOutlierFactor         categorical_category_miss   0.673954   \n",
       "8   LocalOutlierFactorNovelty         categorical_category_miss   0.750000   \n",
       "24                OneClassSVM         categorical_category_miss   0.136364   \n",
       "1             IsolationForest  categorical_distribution_changed   0.498601   \n",
       "17         LocalOutlierFactor  categorical_distribution_changed   0.661667   \n",
       "9   LocalOutlierFactorNovelty  categorical_distribution_changed   0.750000   \n",
       "25                OneClassSVM  categorical_distribution_changed   0.136364   \n",
       "2             IsolationForest   categorical_new_category_influx   0.434449   \n",
       "18         LocalOutlierFactor   categorical_new_category_influx   0.682456   \n",
       "10  LocalOutlierFactorNovelty   categorical_new_category_influx   0.750000   \n",
       "26                OneClassSVM   categorical_new_category_influx   0.136364   \n",
       "3             IsolationForest                 duplicates_influx   0.620376   \n",
       "19         LocalOutlierFactor                 duplicates_influx   0.667995   \n",
       "11  LocalOutlierFactorNovelty                 duplicates_influx   0.880000   \n",
       "27                OneClassSVM                 duplicates_influx   0.157143   \n",
       "4             IsolationForest                        nan_influx   0.397200   \n",
       "20         LocalOutlierFactor                        nan_influx   0.728485   \n",
       "12  LocalOutlierFactorNovelty                        nan_influx   0.847417   \n",
       "28                OneClassSVM                        nan_influx   0.155324   \n",
       "5             IsolationForest               numeric_mean_change   0.481365   \n",
       "21         LocalOutlierFactor               numeric_mean_change   0.862390   \n",
       "13  LocalOutlierFactorNovelty               numeric_mean_change   0.937500   \n",
       "29                OneClassSVM               numeric_mean_change   0.157895   \n",
       "6             IsolationForest           numeric_variance_change   0.506076   \n",
       "22         LocalOutlierFactor           numeric_variance_change   0.829049   \n",
       "14  LocalOutlierFactorNovelty           numeric_variance_change   0.937500   \n",
       "30                OneClassSVM           numeric_variance_change   0.157895   \n",
       "7             IsolationForest                     random_influx   0.439949   \n",
       "23         LocalOutlierFactor                     random_influx   0.740977   \n",
       "15  LocalOutlierFactorNovelty                     random_influx   0.880000   \n",
       "31                OneClassSVM                     random_influx   0.157143   \n",
       "\n",
       "      recall       auc  \n",
       "0   1.000000  0.944375  \n",
       "16  1.000000  0.973750  \n",
       "8   1.000000  0.983333  \n",
       "24  1.000000  0.683333  \n",
       "1   1.000000  0.945000  \n",
       "17  1.000000  0.972292  \n",
       "9   1.000000  0.983333  \n",
       "25  1.000000  0.683333  \n",
       "2   1.000000  0.929375  \n",
       "18  1.000000  0.974792  \n",
       "10  1.000000  0.983333  \n",
       "26  1.000000  0.683333  \n",
       "3   1.000000  0.966250  \n",
       "19  1.000000  0.972500  \n",
       "11  1.000000  0.992500  \n",
       "27  1.000000  0.705000  \n",
       "4   0.929545  0.887148  \n",
       "20  0.751136  0.860318  \n",
       "12  0.763636  0.874318  \n",
       "28  0.986364  0.698182  \n",
       "5   1.000000  0.940268  \n",
       "21  1.000000  0.991161  \n",
       "13  1.000000  0.996429  \n",
       "29  1.000000  0.714286  \n",
       "6   1.000000  0.945893  \n",
       "22  1.000000  0.988571  \n",
       "14  1.000000  0.996429  \n",
       "30  1.000000  0.714286  \n",
       "7   1.000000  0.929875  \n",
       "23  0.901136  0.933318  \n",
       "15  1.000000  0.992500  \n",
       "31  1.000000  0.705000  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_new).sort_values(by=['anomaly', 'algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
