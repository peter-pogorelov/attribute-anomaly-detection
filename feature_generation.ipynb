{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number_repl_isdigit(s):\n",
    "    \"\"\" Returns True if string is a number. \"\"\"\n",
    "    if isinstance(s, str):\n",
    "        return s.replace('.','',1).isdigit()\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def get_num_feature_stats(vals: pd.Series):\n",
    "    is_digit_values = vals.apply(is_number_repl_isdigit)\n",
    "    \n",
    "    vals_digit = vals[is_digit_values].astype('float')\n",
    "    vals_not_digit = vals[~is_digit_values]\n",
    "    \n",
    "    is_nan = vals_digit.isna()\n",
    "    vals_digit_not_nan = vals_digit[~is_nan]\n",
    "    \n",
    "    if not vals_digit_not_nan.empty:\n",
    "        quantiles = np.quantile(vals_digit_not_nan, [0.25, 0.5, 0.75, 0.95])\n",
    "        descr = stats.describe(vals_digit_not_nan)\n",
    "        \n",
    "        return quantiles.tolist() + [\n",
    "            len(vals_digit_not_nan)-len(vals_digit_not_nan.drop_duplicates()),\n",
    "            descr.nobs, \n",
    "            descr.minmax[0], \n",
    "            descr.minmax[1], \n",
    "            descr.mean, \n",
    "            np.sqrt(descr.variance), \n",
    "            descr.skewness, \n",
    "            descr.kurtosis,\n",
    "            is_digit_values.sum() / vals.shape[0],\n",
    "            is_nan.sum() / vals.shape[0]\n",
    "        ]\n",
    "    else:\n",
    "        return [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, (~is_digit_values).sum() / vals.shape[0], is_nan.sum() / vals.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_feature_stats(vals: pd.Series):\n",
    "    is_nan = vals.isna()\n",
    "    vals_not_nan = vals[~is_nan]\n",
    "    \n",
    "    if not vals_not_nan.empty:\n",
    "        vals_counts = vals_not_nan.value_counts()\n",
    "        vals_probas = vals_counts / vals_counts.sum()\n",
    "        entr = entropy(vals_probas.values)    \n",
    "        vals_probas_log = np.log(vals_probas)\n",
    "        descr = stats.describe(vals_probas_log)\n",
    "\n",
    "        # добавить log odds\n",
    "        # добавить count - количество уникальных атрибутов\n",
    "\n",
    "        quantiles = np.quantile(vals_probas_log, [0.25, 0.5, 0.75, 0.95])\n",
    "\n",
    "        return quantiles.tolist() + [\n",
    "            entr, \n",
    "            descr.nobs, \n",
    "            descr.minmax[0], \n",
    "            descr.minmax[1], \n",
    "            is_nan.sum() / vals.shape[0]\n",
    "        ]\n",
    "    else:\n",
    "        [-1,-1,-1,-1,-1,-1,-1,-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_BASE_PATH = \"./Datasets/Real/BankChurners/DirtyBase\"\n",
    "CLEAR_BASE_PATH = \"./Datasets/Real/BankChurners/Clear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = json.load(open('./Datasets/Real/BankChurners/metadata.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_dataframes = list()\n",
    "for fname in sorted(os.listdir(DIRTY_BASE_PATH)):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        dirty_dataframes.append(pd.read_csv(f'{DIRTY_BASE_PATH}/{fname}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_dataframes = list()\n",
    "for fname in sorted(os.listdir(CLEAR_BASE_PATH)):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        clear_dataframes.append(pd.read_csv(f'{CLEAR_BASE_PATH}/{fname}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE GROUP-LEVEL FEATURIZED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_GROUP_PATH = \"./Datasets/Real/BankChurners/DirtyGroup\"\n",
    "DIRTY_GROUP_FEATURIZED_PATH = \"./Datasets/Featurized/BankChurners/DirtyGroup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = {}\n",
    "dataset_control = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anomaly_name in os.listdir(DIRTY_GROUP_PATH):\n",
    "    if anomaly_name.startswith('.'):\n",
    "        continue\n",
    "        \n",
    "    anomaly_path = DIRTY_GROUP_PATH + f\"/{anomaly_name}\"\n",
    "    labels = json.load(open(f\"{anomaly_path}/labels.json\", \"r\"))\n",
    "    \n",
    "    for dataset_name in os.listdir(anomaly_path):\n",
    "        if not dataset_name.endswith(\".csv\"):\n",
    "            continue\n",
    "        df = pd.read_csv(f\"{anomaly_path}/{dataset_name}\")\n",
    "        \n",
    "        for column, metadata in METADATA.items():\n",
    "            if metadata == 'categorical':\n",
    "                column_feature = get_cat_feature_stats(df[column])\n",
    "            elif metadata == 'numeric':\n",
    "                column_feature = get_num_feature_stats(df[column])\n",
    "            else:\n",
    "                raise Exception(f\"Unsupported type of columns: {metadata}\")\n",
    "            \n",
    "            if column not in dataset_control:\n",
    "                dataset_control[column] = list()\n",
    "            \n",
    "            dataset_control[column].append((column_feature, column in labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in clear_dataframes:\n",
    "    for column, metadata in METADATA.items():\n",
    "        column_raw = df[column]\n",
    "        \n",
    "        if metadata == 'categorical':\n",
    "            column_feature = get_cat_feature_stats(pd.Series(column_raw))\n",
    "        elif metadata == 'numeric':\n",
    "            column_feature = get_num_feature_stats(pd.Series(column_raw))\n",
    "        else:\n",
    "            raise Exception(\"unknown column\", column)\n",
    "        \n",
    "        if column not in dataset_train:\n",
    "            dataset_train[column] = list()\n",
    "        dataset_train[column].append((column_feature, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DIRTY_GROUP_FEATURIZED_PATH, exist_ok=True)\n",
    "\n",
    "pickle.dump(\n",
    "    {'train': dataset_train, 'control': dataset_control}, \n",
    "    open(f\"{DIRTY_GROUP_FEATURIZED_PATH}/DirtyGroup.pickle\", \"wb\"), \n",
    "    protocol=pickle.HIGHEST_PROTOCOL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE ANOMALY-LEVEL FEATURIZED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_SINGLE_ANOMALY_PATH = \"./Datasets/Real/BankChurners/DirtySingleAnomaly\"\n",
    "DIRTY_SINGLE_ANOMALY_FEATURIZED_PATH = \"./Datasets/Featurized/BankChurners/DirtySingleAnomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_anomaly = dict()\n",
    "\n",
    "for anomaly_metatype in os.listdir(DIRTY_SINGLE_ANOMALY_PATH):\n",
    "    if anomaly_metatype.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    for anomaly_type in os.listdir(f\"{DIRTY_SINGLE_ANOMALY_PATH}/{anomaly_metatype}\"):\n",
    "        if anomaly_type.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        path_to_columns = f\"{DIRTY_SINGLE_ANOMALY_PATH}/{anomaly_metatype}/{anomaly_type}\"\n",
    "        \n",
    "        dataset_anomaly[anomaly_type] = dict()\n",
    "        \n",
    "        for column in os.listdir(path_to_columns):\n",
    "            if column.startswith('.'):\n",
    "                continue\n",
    "            \n",
    "            dataset_anomaly[anomaly_type][column] = list()\n",
    "            \n",
    "            for dataset_name in os.listdir(f\"{path_to_columns}/{column}\"):\n",
    "                if not dataset_name.endswith(\".npy\"):\n",
    "                    continue\n",
    "\n",
    "                column_raw = np.load(f\"{path_to_columns}/{column}/{dataset_name}\", allow_pickle=True)\n",
    "                \n",
    "                if METADATA[column] == 'categorical':\n",
    "                    column_feature = get_cat_feature_stats(pd.Series(column_raw))\n",
    "                elif METADATA[column] == 'numeric':\n",
    "                    column_feature = get_num_feature_stats(pd.Series(column_raw))\n",
    "                else:\n",
    "                    raise Exception(\"unknown column\", column)\n",
    "                \n",
    "                dataset_anomaly[anomaly_type][column].append(column_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_control = dict()\n",
    "\n",
    "for df in dirty_dataframes:\n",
    "    for column in METADATA.keys():\n",
    "        column_raw = df[column]\n",
    "        \n",
    "        if METADATA[column] == 'categorical':\n",
    "            column_feature = get_cat_feature_stats(pd.Series(column_raw))\n",
    "        elif METADATA[column] == 'numeric':\n",
    "            column_feature = get_num_feature_stats(pd.Series(column_raw))\n",
    "        else:\n",
    "            raise Exception(\"unknown column\", column)\n",
    "        \n",
    "        if column not in dataset_control.keys():\n",
    "            dataset_control[column] = list()\n",
    "        \n",
    "        dataset_control[column].append(column_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dict()\n",
    "\n",
    "for df in clear_dataframes:\n",
    "    for column in METADATA.keys():\n",
    "        column_raw = df[column]\n",
    "        \n",
    "        if METADATA[column] == 'categorical':\n",
    "            column_feature = get_cat_feature_stats(pd.Series(column_raw))\n",
    "        elif METADATA[column] == 'numeric':\n",
    "            column_feature = get_num_feature_stats(pd.Series(column_raw))\n",
    "        else:\n",
    "            raise Exception(\"unknown column\", column)\n",
    "        \n",
    "        if column not in dataset_train.keys():\n",
    "            dataset_train[column] = list()\n",
    "            \n",
    "        dataset_train[column].append(column_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DIRTY_SINGLE_ANOMALY_FEATURIZED_PATH, exist_ok=True)\n",
    "pickle.dump({\n",
    "    'anomaly': dataset_anomaly,\n",
    "    'train': dataset_train,\n",
    "    'control': dataset_control\n",
    "}, open(f\"{DIRTY_SINGLE_ANOMALY_FEATURIZED_PATH}/DirtySingleAnomaly.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE COLUMN-LEVEL FEATURIZED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_SINGLE_COLUMN_PATH = \"./Datasets/Real/BankChurners/DirtySingleColumn\"\n",
    "DIRTY_SINGLE_COLUMN_FEATURIZED_PATH = \"./Datasets/Featurized/BankChurners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = dict()\n",
    "\n",
    "for column_dir in os.listdir(DIRTY_SINGLE_COLUMN_PATH):\n",
    "    if column_dir.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    column_path = f\"{DIRTY_SINGLE_COLUMN_PATH}/{column_dir}\"\n",
    "    \n",
    "    data_column[column_dir] = {'train': list(), 'anomaly': list(), 'control': list()}\n",
    "    \n",
    "    for defaced_column in sorted(os.listdir(column_path)):        \n",
    "        column_raw = np.load(f\"{column_path}/{defaced_column}\", allow_pickle=True)\n",
    "        if METADATA[column_dir] == 'categorical':\n",
    "            column_feature = get_cat_feature_stats(pd.Series(column_raw))\n",
    "        elif METADATA[column_dir] == 'numeric':\n",
    "            column_feature = get_num_feature_stats(pd.Series(column_raw))\n",
    "        else:\n",
    "            raise Exception(\"unknown column\", column_dir)\n",
    "            \n",
    "        data_column[column_dir]['anomaly'].append(column_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dirty_dataframes:\n",
    "    for column in data_column.keys():\n",
    "        column_raw = df[column]\n",
    "        \n",
    "        if METADATA[column] == 'categorical':\n",
    "            column_feature = get_cat_feature_stats(pd.Series(column_raw))\n",
    "        elif METADATA[column] == 'numeric':\n",
    "            column_feature = get_num_feature_stats(pd.Series(column_raw))\n",
    "        else:\n",
    "            raise Exception(\"unknown column\", column)\n",
    "        \n",
    "        data_column[column]['control'].append(column_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in clear_dataframes:\n",
    "    for column in data_column.keys():\n",
    "        column_raw = df[column]\n",
    "        \n",
    "        if METADATA[column] == 'categorical':\n",
    "            column_feature = get_cat_feature_stats(pd.Series(column_raw))\n",
    "        elif METADATA[column] == 'numeric':\n",
    "            column_feature = get_num_feature_stats(pd.Series(column_raw))\n",
    "        else:\n",
    "            raise Exception(\"unknown column\", column)\n",
    "        \n",
    "        data_column[column]['train'].append(column_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DIRTY_SINGLE_COLUMN_FEATURIZED_PATH, exist_ok=True)\n",
    "pickle.dump(data_column, open(f\"{DIRTY_SINGLE_COLUMN_FEATURIZED_PATH}/DirtySingleColumn.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
